{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/a406127/anaconda3/envs/ctr_prediction/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import skopt\n",
    "import xlearn as xl\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score)\n",
    "from skopt.plots import (plot_convergence, plot_evaluations, plot_objective)\n",
    "from utils.measuring_performance import *\n",
    "from utils.misc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.abspath('../../Data/display_advertising_challenge/processed')\n",
    "MODEL_DIR = os.path.abspath('model')\n",
    "USE_FIELD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'ffm' if USE_FIELD else 'fm'\n",
    "train_dataset_type = 'train'\n",
    "test_dataset_type = 'valid'\n",
    "\n",
    "train_dataset_path = os.path.join(DATA_DIR, '_'.join([model_type, 'dataset', train_dataset_type]) + '.libsvm')\n",
    "test_dataset_path = os.path.join(DATA_DIR, '_'.join([model_type, 'dataset', test_dataset_type]) + '.libsvm')\n",
    "model_path = os.path.join(MODEL_DIR, '_'.join([model_type, 'model', train_dataset_type]) + '.out')\n",
    "score_path = os.path.join(MODEL_DIR, '_'.join([model_type, 'score', test_dataset_type]) + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xl.create_ffm() if USE_FIELD else xl.create_fm()\n",
    "model.setOnDisk()\n",
    "model.setSigmoid()\n",
    "model.setTrain(train_dataset_path)\n",
    "model.setValidate(test_dataset_path)\n",
    "model.setTest(test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(params):\n",
    "    default_params = {\n",
    "        'task': 'binary'\n",
    "        , 'nthread': int(0.5 * multiprocessing.cpu_count())\n",
    "        , 'opt': 'adagrad'\n",
    "        , 'epoch': 100\n",
    "        , 'stop_window': 3\n",
    "        , 'metric': 'auc'\n",
    "    }\n",
    "    default_params.update(params)\n",
    "    return default_params\n",
    "\n",
    "\n",
    "def set_args(model, space, model_path, score_path, y_true):\n",
    "    @skopt.utils.use_named_args(space)\n",
    "    def _objective(**params):\n",
    "        params = update_params(params)\n",
    "        model.fit(params, model_path)\n",
    "        model.predict(model_path, score_path)\n",
    "        with open(score_path, 'r') as file:\n",
    "            y_score = np.array([float(line) for line in file])\n",
    "        return -1.0 * roc_auc_score(y_true, y_score)\n",
    "    return _objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_dataset_path, 'r') as file:\n",
    "    y_true = np.array([int(line[0]) for line in file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [skopt.space.Real(0.0125, 0.4, name='lr', prior='log-uniform'), \n",
    "         skopt.space.Real(0.0005, 0.016, name='lambda', prior='log-uniform'), \n",
    "         skopt.space.Integer(2, 32, name='k')]\n",
    "\n",
    "objective = set_args(model, space, model_path, score_path, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration No: 1 started. Evaluating function at random point.\n",
      "Iteration No: 1 ended. Evaluation done at random point.\n",
      "Time taken: 117.2049\n",
      "Function value obtained: -0.7154\n",
      "Current minimum: -0.7154\n",
      "Iteration No: 2 started. Evaluating function at random point.\n",
      "Iteration No: 2 ended. Evaluation done at random point.\n",
      "Time taken: 142.9001\n",
      "Function value obtained: -0.7219\n",
      "Current minimum: -0.7219\n",
      "Iteration No: 3 started. Evaluating function at random point.\n",
      "Iteration No: 3 ended. Evaluation done at random point.\n",
      "Time taken: 250.6551\n",
      "Function value obtained: -0.7239\n",
      "Current minimum: -0.7239\n",
      "Iteration No: 4 started. Evaluating function at random point.\n",
      "Iteration No: 4 ended. Evaluation done at random point.\n",
      "Time taken: 128.3695\n",
      "Function value obtained: -0.7124\n",
      "Current minimum: -0.7239\n",
      "Iteration No: 5 started. Evaluating function at random point.\n",
      "Iteration No: 5 ended. Evaluation done at random point.\n",
      "Time taken: 324.0491\n",
      "Function value obtained: -0.7203\n",
      "Current minimum: -0.7239\n",
      "Iteration No: 6 started. Evaluating function at random point.\n",
      "Iteration No: 6 ended. Evaluation done at random point.\n",
      "Time taken: 195.6585\n",
      "Function value obtained: -0.7255\n",
      "Current minimum: -0.7255\n",
      "Iteration No: 7 started. Evaluating function at random point.\n",
      "Iteration No: 7 ended. Evaluation done at random point.\n",
      "Time taken: 174.0961\n",
      "Function value obtained: -0.7145\n",
      "Current minimum: -0.7255\n",
      "Iteration No: 8 started. Evaluating function at random point.\n",
      "Iteration No: 8 ended. Evaluation done at random point.\n",
      "Time taken: 168.9413\n",
      "Function value obtained: -0.7236\n",
      "Current minimum: -0.7255\n",
      "Iteration No: 9 started. Evaluating function at random point.\n",
      "Iteration No: 9 ended. Evaluation done at random point.\n",
      "Time taken: 168.6972\n",
      "Function value obtained: -0.7252\n",
      "Current minimum: -0.7255\n",
      "Iteration No: 10 started. Evaluating function at random point.\n",
      "Iteration No: 10 ended. Evaluation done at random point.\n",
      "Time taken: 116.8515\n",
      "Function value obtained: -0.7226\n",
      "Current minimum: -0.7255\n",
      "Iteration No: 11 started. Searching for the next optimal point.\n",
      "Iteration No: 11 ended. Search finished for the next optimal point.\n",
      "Time taken: 126.6903\n",
      "Function value obtained: -0.7227\n",
      "Current minimum: -0.7255\n",
      "Iteration No: 12 started. Searching for the next optimal point.\n",
      "Iteration No: 12 ended. Search finished for the next optimal point.\n",
      "Time taken: 134.3963\n",
      "Function value obtained: -0.7214\n",
      "Current minimum: -0.7255\n",
      "Iteration No: 13 started. Searching for the next optimal point.\n"
     ]
    }
   ],
   "source": [
    "with get_elapsed_time():\n",
    "    results = skopt.forest_minimize(objective, space, base_estimator='ET', acq_func='EI', n_calls=30, \n",
    "                                    random_state=42, verbose=True, xi=0.01, n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skopt.dump(results, os.path.join(MODEL_DIR, '_'.join([model_type, 'opt', train_dataset_type]) + '.pkl'), \n",
    "           store_objective=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = skopt.plots.plot_convergence(results)\n",
    "_ = skopt.plots.plot_evaluations(results)\n",
    "_ = skopt.plots.plot_objective(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = skopt.load(os.path.join(MODEL_DIR, '_'.join([model_type, 'opt', train_dataset_type]) + '.pkl'))\n",
    "\n",
    "train_dataset_type = 'train+valid'\n",
    "test_dataset_type = 'test'\n",
    "\n",
    "train_dataset_path = os.path.join(DATA_DIR, '_'.join([model_type, 'dataset', train_dataset_type]) + '.libsvm')\n",
    "test_dataset_path = os.path.join(DATA_DIR, '_'.join([model_type, 'dataset', test_dataset_type]) + '.libsvm')\n",
    "model_path = os.path.join(MODEL_DIR, '_'.join([model_type, 'model', train_dataset_type]) + '.out')\n",
    "score_path = os.path.join(MODEL_DIR, '_'.join([model_type, 'score', test_dataset_type]) + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setTrain(train_dataset_path)\n",
    "model.setValidate(test_dataset_path)\n",
    "model.setTest(test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_dataset_path, 'r') as file:\n",
    "    y_true = np.array([int(line[0]) for line in file])\n",
    "ctr = y_true.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_elapsed_time():\n",
    "    model.fit(update_params({k: v for k, v in zip(['lr', 'lambda', 'k'], results.x)}), model_path)\n",
    "    model.predict(model_path, score_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(score_path, 'r') as file:\n",
    "    y_score = np.array([float(line) for line in file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = get_y_pred(y_score, ctr)\n",
    "norm_entropy = get_norm_entropy(y_true, y_score)\n",
    "calibration = y_score.mean() / ctr\n",
    "accuracy, precision, recall, f1 = accuracy_score(y_true, y_pred), precision_score(y_true, y_pred), \\\n",
    "recall_score(y_true, y_pred), f1_score(y_true, y_pred)\n",
    "confusion_matrix = plot_confusion_matrix(y_true, y_pred)\n",
    "auroc = plot_roc_curve(y_true, y_score)\n",
    "auprc = plot_pr_curve(y_true, y_score)\n",
    "_ = plot_lift_curve(y_true, y_score)\n",
    "_ = plot_class_density(y_true, y_score, threshold=ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_pickle(os.path.join(MODEL_DIR, '_'.join([model_type, 'metric', train_dataset_type]) + '.pkl'), \n",
    "            (norm_entropy, calibration, accuracy, precision, recall, f1, confusion_matrix, auroc, auprc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_type = 'quiz'\n",
    "test_dataset_path = os.path.join(DATA_DIR, '_'.join([model_type, 'dataset', test_dataset_type]) + '.libsvm')\n",
    "score_path = os.path.join(MODEL_DIR, '_'.join([model_type, 'score', test_dataset_type]) + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.setTest(test_dataset_path)\n",
    "with get_elapsed_time():\n",
    "    model.predict(model_path, score_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
