{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f28a5e6e1c72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringIndexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringIndexerModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuantileDiscretizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StandardScaler, StringIndexer, StringIndexerModel, QuantileDiscretizer, VectorAssembler\n",
    "import hyspark\n",
    "from my_utils.misc import *\n",
    "from my_utils.spark_utils import *\n",
    " \n",
    " \n",
    "def get_dataset_for_train(hc, use_three_folds=True, use_time_based=True, database=None):\n",
    "    dataset_types = ('train', 'valid') if use_three_folds else ('train\\', \\'valid', 'test')\n",
    "    dataset_table = 'fm_time_based_feature_for_train' if use_time_based else 'fm_user_based_feature_for_train'\n",
    "    datasets = []\n",
    "    for dataset_type in dataset_types:\n",
    "        query = 'SELECT * FROM {}.{} WHERE dataset_type IN (\\'{}\\')'.format(database, dataset_table, dataset_type)\n",
    "        cleaned_query = get_cleaned_hql(query)\n",
    "        datasets.append(hc.sql(cleaned_query))\n",
    "    return datasets[0], datasets[1]\n",
    " \n",
    " \n",
    "def get_feature_name(hc, category=None, stat_data_type=None, database=None):\n",
    "    query = 'SELECT * FROM %s.fm_feature_metadata' % database\n",
    "    cleaned_query = get_cleaned_hql(query)\n",
    "    feature_names = hc.sql(cleaned_query)\n",
    " \n",
    "    if category is not None:\n",
    "        feature_names = feature_names.filter(F.col('category').isin(category))\n",
    "    if stat_data_type is not None:\n",
    "        feature_names = feature_names.filter(F.col('stat_data_type').isin(stat_data_type))\n",
    "    return set(feature_names.select('name').rdd.flatMap(lambda x: x).collect())\n",
    " \n",
    " \n",
    "def set_feature_name(key_names, target_name, cat_feature_names, num_feature_names, feature_name_to_index,\n",
    "                     is_training=True, use_field=False, decimals=6):\n",
    "    def _parse_row(row):\n",
    "        key = ','.join([str(row[key_name]) for key_name in key_names])\n",
    "        value = str(row[target_name]) if is_training else ''\n",
    "        for feature_name in list(cat_feature_names):\n",
    "            try:\n",
    "                if use_field:\n",
    "                    feature_index = feature_name_to_index[feature_name]\n",
    "                    value = ''.join(\n",
    "                        [value, ' ', str(feature_index), ':', str(int(row[feature_name])), ':1'])\n",
    "                else:\n",
    "                    feature_index = feature_name_to_index[''.join([feature_name, '_', str(int(row[feature_name]))])]\n",
    "                    value = ''.join([value, ' ', str(feature_index), ':1'])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        for i, feature_name in enumerate(list(num_feature_names)):\n",
    "            try:\n",
    "                feature_index = feature_name_to_index[feature_name]\n",
    "                field = str(feature_index) + ':' if use_field else ''\n",
    "                value = ''.join([value, ' ', str(feature_index), ':', field,\n",
    "                                 str(round(row['scaled_num_features'][i], decimals))])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return key, value.lstrip()\n",
    " \n",
    "    return _parse_row\n",
    " \n",
    " \n",
    "APP_NAME = 'building_xlearn_dataset_for_train'\n",
    "MEM_PER_CORE = 10\n",
    "INSTANCE = 'full'\n",
    " \n",
    "USE_THREE_FOLDS = True\n",
    "USE_TIME_BASED = True\n",
    "USE_FIELD = False\n",
    "SAMPLING_RATE = 0.1\n",
    "IS_NEGATIVE_DOWN_SAMPLING = False\n",
    "FEATURE_CATEGORIES = sorted(['user_demo', 'user_dtag', 'item_profile', 'item_context'])\n",
    " \n",
    "HADOOP_DATA_DIR_NAME = 'data'\n",
    "DATABASE = 'hcc_big_brain_lv2'\n",
    " \n",
    " \n",
    "def build_xlearn_dataset_for_train():\n",
    "    sc = hyspark.HySpark(APP_NAME, MEM_PER_CORE, instance=INSTANCE)\n",
    "    hc = sc.hive_context\n",
    " \n",
    "    train_df, test_df = get_dataset_for_train(hc, use_three_folds=USE_THREE_FOLDS, use_time_based=USE_TIME_BASED,\n",
    "                                              database=DATABASE)\n",
    "    train_sampling_rate = SAMPLING_RATE\n",
    "    test_sampling_rate = SAMPLING_RATE\n",
    "    target_name = 'click_yn'\n",
    "    sampled_train_df = sample_dataset(\n",
    "        train_df, train_sampling_rate, is_negative_down_sampling=IS_NEGATIVE_DOWN_SAMPLING,\n",
    "        target_name=target_name).persist()\n",
    "    sampled_test_df = sample_dataset(\n",
    "        test_df, test_sampling_rate, is_negative_down_sampling=False).persist()\n",
    " \n",
    "    with get_elapsed_time():\n",
    "        train_count = sampled_train_df.count()\n",
    "        train_target_ratio = sampled_train_df.groupBy(target_name).count().collect()[0]['count'] / train_count\n",
    "        test_count = sampled_test_df.count()\n",
    "        test_target_ratio = sampled_test_df.groupBy(target_name).count().collect()[0]['count'] / test_count\n",
    " \n",
    "    print('Num of obs. in training set: {}\\nTarget Ratio in training set: {:.4%}\\nNum of obs. in test set: {}\\n\\\n",
    "    Target Ratio in test set: {:.4%}'.format(train_count, train_target_ratio, test_count, test_target_ratio))\n",
    " \n",
    "    cat_feature_names = get_feature_name(\n",
    "        hc, category=FEATURE_CATEGORIES, stat_data_type=['binary', 'categorical'], database=DATABASE)\n",
    "    num_feature_names = get_feature_name(\n",
    "        hc, category=FEATURE_CATEGORIES, stat_data_type=['numerical'], database=DATABASE)\n",
    "    all_feature_names = cat_feature_names | num_feature_names\n",
    " \n",
    "    print('Num of cat features:', len(cat_feature_names), '\\nNum of num features:', len(num_feature_names))\n",
    " \n",
    "    with get_elapsed_time():\n",
    "        null_ratios = pd.Series(get_null_ratio(sampled_train_df, all_feature_names))\n",
    "        null_ratio_thr = 0.9\n",
    "        dropped_feature_names = set(null_ratios[null_ratios > null_ratio_thr].index)\n",
    " \n",
    "    print('Num of features to be removed:', len(dropped_feature_names))\n",
    " \n",
    "    cat_feature_names = cat_feature_names.difference(dropped_feature_names)\n",
    "    num_feature_names = num_feature_names.difference(dropped_feature_names)\n",
    "    all_feature_names = all_feature_names.difference(dropped_feature_names)\n",
    " \n",
    "    print('Num of cat features:', len(cat_feature_names), '\\nNum of num features:', len(num_feature_names))\n",
    " \n",
    "    key_names = ['csno', 'cntn_id', 'event_dttm']\n",
    "    sampled_train_df = sampled_train_df.select(key_names + [target_name] + list(all_feature_names))\n",
    "    sampled_train_df = cast_data_type(sampled_train_df, cat_feature_names, 'string')\n",
    "    sampled_train_df = cast_data_type(sampled_train_df, num_feature_names, 'double')\n",
    "    sampled_train_df = sampled_train_df.fillna('unknown', list(cat_feature_names)).fillna(0.0, list(num_feature_names))\n",
    "    sampled_test_df = sampled_test_df.select(key_names + [target_name] + list(all_feature_names))\n",
    "    sampled_test_df = cast_data_type(sampled_test_df, cat_feature_names, 'string')\n",
    "    sampled_test_df = cast_data_type(sampled_test_df, num_feature_names, 'double')\n",
    "    sampled_test_df = sampled_test_df.fillna('unknown', list(cat_feature_names)).fillna(0.0, list(num_feature_names))\n",
    " \n",
    "    transformers = []\n",
    "    for feature_name in cat_feature_names:\n",
    "        indexer = StringIndexer(handleInvalid='keep').setInputCol(feature_name).setOutputCol(\n",
    "            '_'.join(['indexed', feature_name]))\n",
    "        transformers.append(indexer)\n",
    " \n",
    "    discretize = False\n",
    "    if discretize:\n",
    "        n_buckets = 4\n",
    "        for feature_name in num_feature_names:\n",
    "            discretizer = QuantileDiscretizer(numBuckets=n_buckets, handleInvalid='keep').setInputCol(\n",
    "                feature_name).setOutputCol('_'.join(['discretized', feature_name]))\n",
    "            transformers.append(discretizer)\n",
    "            indexer = StringIndexer(handleInvalid='keep').setInputCol(\n",
    "                '_'.join(['discretized', feature_name])).setOutputCol(\n",
    "                '_'.join(['indexed', feature_name]))\n",
    "            transformers.append(indexer)\n",
    "    else:\n",
    "        transformers.extend([VectorAssembler().setInputCols(list(num_feature_names)).setOutputCol('num_features'),\n",
    "                             AsDenseTransformer().setInputCol('num_features').setOutputCol('num_features'),\n",
    "                             StandardScaler(withMean=True, withStd=True).setInputCol(\n",
    "                                 'num_features').setOutputCol('scaled_num_features')])\n",
    " \n",
    "    with get_elapsed_time():\n",
    "        pipeline = Pipeline().setStages(transformers)\n",
    "        fitted_pipeline = pipeline.fit(sampled_train_df)\n",
    "        sampled_train_df = fitted_pipeline.transform(sampled_train_df)\n",
    "        sampled_test_df = fitted_pipeline.transform(sampled_test_df)\n",
    " \n",
    "    all_transformed_feature_names = pd.Series(sampled_train_df.columns)\n",
    "    all_transformed_feature_names = set(\n",
    "        all_transformed_feature_names[all_transformed_feature_names.str.startswith('indexed_')].tolist())\n",
    "    cat_transformed_feature_names = all_transformed_feature_names\n",
    "    num_transformed_feature_names = set() if discretize else num_feature_names\n",
    "    all_transformed_feature_names = all_transformed_feature_names if discretize else \\\n",
    "        all_transformed_feature_names | {'scaled_num_features'}\n",
    "    sampled_train_df = sampled_train_df.select(key_names + [target_name] + list(all_transformed_feature_names))\n",
    "    sampled_test_df = sampled_test_df.select(key_names + [target_name] + list(all_transformed_feature_names))\n",
    " \n",
    "    feature_name_to_index = {}\n",
    "    if USE_FIELD:\n",
    "        for i, feature_name in enumerate(list(cat_transformed_feature_names | num_transformed_feature_names)):\n",
    "            feature_name_to_index[feature_name] = i\n",
    "    else:\n",
    "        indexed_labels_of_cat_features = [(x._java_obj.getOutputCol(), list(range(len(x.labels))))\n",
    "                                          for x in fitted_pipeline.stages if isinstance(x, StringIndexerModel)]\n",
    "        i = 0\n",
    "        for indexed_labels in indexed_labels_of_cat_features:\n",
    "            for indexed_label in indexed_labels[1]:\n",
    "                feature_name = '_'.join([indexed_labels[0], str(indexed_label)])\n",
    "                feature_name_to_index[feature_name] = i\n",
    "                i += 1\n",
    "        for j, feature_name in enumerate(list(num_feature_names)):\n",
    "            feature_name_to_index[feature_name] = i + j\n",
    " \n",
    "    print('Num of features:', len(feature_name_to_index))\n",
    " \n",
    "    parse_row = set_feature_name(key_names, target_name, cat_transformed_feature_names, num_transformed_feature_names,\n",
    "                                 feature_name_to_index, use_field=USE_FIELD, decimals=6)\n",
    " \n",
    "    sampled_train_dataset = sampled_train_df.rdd.map(lambda x: parse_row(x)[1])\n",
    "    sampled_test_dataset = sampled_test_df.rdd.map(lambda x: parse_row(x)[1])\n",
    " \n",
    "    train_set_type = 'train' if USE_THREE_FOLDS else 'train+valid'\n",
    "    test_set_type = 'valid' if USE_THREE_FOLDS else 'test'\n",
    "    source_table = 'time' if USE_TIME_BASED else 'user'\n",
    "    model_type = 'ffm' if USE_FIELD else 'fm'\n",
    "    train_sampling_type = '%dbp_neg_sampled_' % int(\n",
    "        10000 * SAMPLING_RATE) if IS_NEGATIVE_DOWN_SAMPLING else '%dbp_sampled_' % int(10000 * SAMPLING_RATE)\n",
    "    test_sampling_type = '%dbp_sampled_' % int(\n",
    "        10000 * SAMPLING_RATE) if USE_THREE_FOLDS else ''\n",
    "    feature_categories = ['all'] if FEATURE_CATEGORIES is None else FEATURE_CATEGORIES\n",
    " \n",
    "    _ = mkdir_in_hdfs(HADOOP_DATA_DIR_NAME)\n",
    " \n",
    "    fitted_pipeline_name = os.path.join(HADOOP_DATA_DIR_NAME, 'fm_{}_based_{}_{}_pipeline({})'.format(\n",
    "        source_table, train_sampling_type, train_set_type, '+'.join(feature_categories)))\n",
    "    sampled_train_dataset_name = os.path.join(HADOOP_DATA_DIR_NAME, '{}_{}_based_{}_{}_dataset({})'.format(\n",
    "        model_type, source_table, train_sampling_type, train_set_type, '+'.join(feature_categories)))\n",
    "    sampled_test_dataset_name = os.path.join(HADOOP_DATA_DIR_NAME, '{}_{}_based_{}_{}_dataset({})'.format(\n",
    "        model_type, source_table, test_sampling_type, test_set_type, '+'.join(feature_categories)))\n",
    " \n",
    "    with get_elapsed_time():\n",
    "        n_partitions = 3 * get_total_n_executors_core(sc.spark_context)\n",
    " \n",
    "        _ = delete_file_in_hdfs(fitted_pipeline_name)\n",
    "        _ = delete_file_in_hdfs(sampled_train_dataset_name)\n",
    "        _ = delete_file_in_hdfs(sampled_test_dataset_name)\n",
    " \n",
    "        fitted_pipeline.write().save(fitted_pipeline_name)\n",
    "        sampled_train_dataset.coalesce(n_partitions).saveAsTextFile(sampled_train_dataset_name)\n",
    "        sampled_test_dataset.coalesce(n_partitions).saveAsTextFile(sampled_test_dataset_name)\n",
    " \n",
    "    metadata_name = os.path.join(HADOOP_DATA_DIR_NAME, '{}_{}_based_{}_{}_metadata({}).pkl'.format(\n",
    "        model_type, source_table, train_sampling_type, train_set_type, '+'.join(feature_categories)))\n",
    " \n",
    "    dump_pickle('temp.pkl', (key_names, target_name, all_feature_names, cat_feature_names, num_feature_names,\n",
    "                             all_transformed_feature_names, cat_transformed_feature_names,\n",
    "                             num_transformed_feature_names, feature_name_to_index, train_target_ratio))\n",
    " \n",
    "    _ = delete_file_in_hdfs(metadata_name)\n",
    "    _ = copy_local_to_hdfs('temp.pkl', metadata_name)\n",
    "    _ = delete_dir_or_file('temp.pkl')\n",
    " \n",
    "    sc.stop()\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    build_xlearn_dataset_for_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from my_utils.misc import *\n",
    " \n",
    " \n",
    "USE_THREE_FOLDS = True\n",
    "USE_TIME_BASED = True\n",
    "USE_FIELD = False\n",
    "SAMPLING_RATE = 0.1\n",
    "IS_NEGATIVE_DOWN_SAMPLING = False\n",
    "FEATURE_CATEGORIES = sorted(['user_demo', 'user_dtag', 'item_profile', 'item_context'])\n",
    " \n",
    "DATA_DIR_NAME = 'data'\n",
    "HADOOP_DATA_DIR_NAME = 'data'\n",
    " \n",
    " \n",
    "def prepare_for_xlearn_train():\n",
    "    train_set_type = 'train' if USE_THREE_FOLDS else 'train+valid'\n",
    "    test_set_type = 'valid' if USE_THREE_FOLDS else 'test'\n",
    "    train_sampling_type = '%dbp_neg_sampled' % int(\n",
    "        10000 * SAMPLING_RATE) if IS_NEGATIVE_DOWN_SAMPLING else '%dbp_sampled' % int(10000 * SAMPLING_RATE)\n",
    "    test_sampling_type = '%dbp_sampled' % int(\n",
    "        10000 * SAMPLING_RATE) if IS_NEGATIVE_DOWN_SAMPLING else '%dbp_sampled' % int(10000 * SAMPLING_RATE)\n",
    "    source_table = 'time' if USE_TIME_BASED else 'user'\n",
    "    model_type = 'ffm' if USE_FIELD else 'fm'\n",
    "    feature_categories = ['all'] if FEATURE_CATEGORIES is None else FEATURE_CATEGORIES\n",
    " \n",
    "    train_set_name = '{}_{}_based_{}_{}_dataset({})'.format(\n",
    "        model_type, source_table, train_sampling_type, train_set_type, '+'.join(feature_categories))\n",
    "    test_set_name = '{}_{}_based_{}_{}_dataset({})'.format(\n",
    "        model_type, source_table, test_sampling_type, test_set_type, '+'.join(feature_categories))\n",
    " \n",
    "    with get_elapsed_time():\n",
    "        if not os.path.exists(DATA_DIR_NAME):\n",
    "            os.mkdir(DATA_DIR_NAME)\n",
    " \n",
    "        _ = delete_dir_or_file(os.path.join(DATA_DIR_NAME, train_set_name))\n",
    "        _ = delete_dir_or_file(os.path.join(DATA_DIR_NAME, test_set_name))\n",
    "        _ = delete_dir_or_file(os.path.join(DATA_DIR_NAME, train_set_name + '.libsvm'))\n",
    "        _ = delete_dir_or_file(os.path.join(DATA_DIR_NAME, test_set_name + '.libsvm'))\n",
    " \n",
    "        _ = copy_hdfs_to_local(os.path.join(HADOOP_DATA_DIR_NAME, train_set_name),\n",
    "                               os.path.join(DATA_DIR_NAME, train_set_name))\n",
    "        _ = copy_hdfs_to_local(os.path.join(HADOOP_DATA_DIR_NAME, test_set_name),\n",
    "                               os.path.join(DATA_DIR_NAME, test_set_name))\n",
    " \n",
    "        concat_text_file(sorted(glob.glob(os.path.join(DATA_DIR_NAME, train_set_name, 'part-*'))), os.path.join(\n",
    "            DATA_DIR_NAME, train_set_name + '.libsvm'))\n",
    "        concat_text_file(sorted(glob.glob(os.path.join(DATA_DIR_NAME, test_set_name, 'part-*'))), os.path.join(\n",
    "            DATA_DIR_NAME, test_set_name + '.libsvm'))\n",
    " \n",
    "        _ = delete_dir_or_file(os.path.join(DATA_DIR_NAME, train_set_name))\n",
    "        _ = delete_dir_or_file(os.path.join(DATA_DIR_NAME, test_set_name))\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    prepare_for_xlearn_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import PipelineModel\n",
    "import hyspark\n",
    "from my_utils.misc import *\n",
    "from my_utils.spark_utils import *\n",
    " \n",
    " \n",
    "def delete_dir_or_file(path):\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "    except:\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except:\n",
    "            pass\n",
    " \n",
    " \n",
    "def get_dataset_for_service(hc, is_training=True, use_time_based=True, database=None):\n",
    "    if is_training:\n",
    "        dataset_table = 'fm_time_based_feature_for_train' if use_time_based else 'fm_user_based_feature_for_train'\n",
    "        dataset_type = 'test'\n",
    "    else:\n",
    "        pass\n",
    " \n",
    "    query = 'SELECT * FROM {}.{} WHERE dataset_type = \\'{}\\''.format(database, dataset_table, dataset_type)\n",
    "    cleaned_query = get_cleaned_hql(query)\n",
    "    dataset = hc.sql(cleaned_query)\n",
    "    return dataset\n",
    " \n",
    " \n",
    "def set_feature_name(key_names, target_name, cat_feature_names, num_feature_names, feature_name_to_index,\n",
    "                     is_training=True, use_field=False, decimals=6):\n",
    "    def _parse_row(row):\n",
    "        key = ','.join([str(row[key_name]) for key_name in key_names])\n",
    "        value = str(row[target_name]) if is_training else ''\n",
    "        for feature_name in list(cat_feature_names):\n",
    "            try:\n",
    "                if use_field:\n",
    "                    feature_index = feature_name_to_index[feature_name]\n",
    "                    value = ''.join(\n",
    "                        [value, ' ', str(feature_index), ':', str(int(row[feature_name])), ':1'])\n",
    "                else:\n",
    "                    feature_index = feature_name_to_index[''.join([feature_name, '_', str(int(row[feature_name]))])]\n",
    "                    value = ''.join([value, ' ', str(feature_index), ':1'])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        for i, feature_name in enumerate(list(num_feature_names)):\n",
    "            try:\n",
    "                feature_index = feature_name_to_index[feature_name]\n",
    "                field = str(feature_index) + ':' if use_field else ''\n",
    "                value = ''.join([value, ' ', str(feature_index), ':', field,\n",
    "                                 str(round(row['scaled_num_features'][i], decimals))])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        return key, value.lstrip()\n",
    " \n",
    "    return _parse_row\n",
    " \n",
    " \n",
    "def set_model_conf(is_training=True, use_field=False, disable_norm=True):\n",
    "    def _predict(rows):\n",
    "        import xlearn as xl\n",
    " \n",
    "        user_ids, item_ids, event_times, values, labels = [], [], [], [], []\n",
    "        for key, value in rows:\n",
    "            user_id, item_id, event_time = key.split(',')\n",
    "            label = int(value[0]) if is_training else None\n",
    "            user_ids.append(user_id)\n",
    "            item_ids.append(item_id)\n",
    "            event_times.append(event_time)\n",
    "            values.append(value)\n",
    "            labels.append(label)\n",
    " \n",
    "        model = xl.create_fm() if use_field else xl.create_fm()\n",
    "        if disable_norm:\n",
    "            model.disableNorm()\n",
    "        model.setSigmoid()\n",
    " \n",
    "        uid = str(uuid.uuid4())\n",
    "        data_file_name = 'temp-%s.libsvm' % uid\n",
    "        score_file_name = 'temp-%s.txt' % uid\n",
    "        with open(data_file_name, mode='wt') as f:\n",
    "            f.write('\\n'.join(values))\n",
    "        model.setTest(data_file_name)\n",
    "        model.predict('temp.out', score_file_name)\n",
    "        with open(score_file_name, mode='rt') as f:\n",
    "            scores = [float(line) for line in f]\n",
    "        _ = delete_dir_or_file(data_file_name)\n",
    "        _ = delete_dir_or_file(score_file_name)\n",
    "        return zip(user_ids, item_ids, event_times, scores, labels)\n",
    " \n",
    "    return _predict\n",
    " \n",
    " \n",
    "APP_NAME = 'building_xlearn_dataset_for_service'\n",
    "MEM_PER_CORE = 10\n",
    "INSTANCE = 'full'\n",
    " \n",
    "IS_TRAINING = True\n",
    "BASE_DATE = '20191001'\n",
    " \n",
    "USE_TIME_BASED = True\n",
    "USE_FIELD = False\n",
    "TRAIN_SAMPLING_RATE = 0.1\n",
    "IS_NEGATIVE_DOWN_SAMPLING = False\n",
    "FEATURE_CATEGORIES = sorted(['user_demo', 'user_dtag', 'item_profile', 'item_context'])\n",
    " \n",
    "HADOOP_DATA_DIR_NAME = 'data'\n",
    "HADOOP_MODEL_DIR_NAME = 'model'\n",
    "IS_APPENDING = False\n",
    "DATABASE = 'hcc_big_brain_lv2'\n",
    " \n",
    " \n",
    "def build_xlearn_dataset_for_service():\n",
    "    sc = hyspark.HySpark(APP_NAME, MEM_PER_CORE, instance=INSTANCE)\n",
    "    hc = sc.hive_context\n",
    "    hc.setConf(\"hive.exec.dynamic.partition\", \"true\")\n",
    "    hc.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n",
    " \n",
    "    service_df = get_dataset_for_service(hc, is_training=IS_TRAINING, use_time_based=USE_TIME_BASED, database=DATABASE)\n",
    " \n",
    "    train_set_type = 'train+valid'\n",
    "    source_table = 'time' if USE_TIME_BASED else 'user'\n",
    "    model_type = 'ffm' if USE_FIELD else 'fm'\n",
    "    train_sampling_type = '%dbp_neg_sampled' % int(\n",
    "        10000 * TRAIN_SAMPLING_RATE) if IS_NEGATIVE_DOWN_SAMPLING else '%dbp_sampled' % int(10000 * TRAIN_SAMPLING_RATE)\n",
    "    feature_categories = ['all'] if FEATURE_CATEGORIES is None else FEATURE_CATEGORIES\n",
    " \n",
    "    metadata_name = os.path.join(HADOOP_DATA_DIR_NAME, '{}_{}_based_{}_{}_metadata({}).pkl'.format(\n",
    "        model_type, source_table, train_sampling_type, train_set_type, '+'.join(feature_categories)))\n",
    "    _ = copy_hdfs_to_local(metadata_name, 'temp.pkl')\n",
    "    key_names, target_name, all_feature_names, cat_feature_names, num_feature_names, all_transformed_feature_names, \\\n",
    "        cat_transformed_feature_names, num_transformed_feature_names, feature_name_to_index, train_target_ratio = \\\n",
    "        load_pickle('temp.pkl')\n",
    "    _ = delete_dir_or_file('temp.pkl')\n",
    " \n",
    "    with get_elapsed_time():\n",
    "        fitted_pipeline_name = os.path.join(HADOOP_DATA_DIR_NAME, '{}_{}_based_{}_{}_pipeline({})'.format(\n",
    "            model_type, source_table, train_sampling_type, train_set_type, '+'.join(feature_categories)))\n",
    "        fitted_pipeline = PipelineModel.load(fitted_pipeline_name)\n",
    " \n",
    "    service_df = service_df.select(key_names + [target_name] + list(all_feature_names))\n",
    "    service_df = cast_data_type(service_df, cat_feature_names, 'string')\n",
    "    service_df = cast_data_type(service_df, num_feature_names, 'double')\n",
    "    service_df = service_df.fillna('unknown', list(cat_feature_names)).fillna(0.0, list(num_feature_names))\n",
    "    service_df = fitted_pipeline.transform(service_df)\n",
    "    service_df = service_df.select(key_names + [target_name] + list(all_transformed_feature_names))\n",
    " \n",
    "    parse_row = set_feature_name(key_names, target_name, cat_transformed_feature_names, num_transformed_feature_names,\n",
    "                                 feature_name_to_index, use_field=USE_FIELD, decimals=6)\n",
    " \n",
    "    service_dataset = service_df.rdd.map(lambda x: parse_row(x))\n",
    " \n",
    "    model_name = '{}_{}_based_{}_{}_model({})'.format(\n",
    "        model_type, source_table, train_sampling_type, train_set_type, '+'.join(feature_categories))\n",
    "    _ = copy_hdfs_to_local(os.path.join(HADOOP_MODEL_DIR_NAME, model_name + '.out'), 'temp.out')\n",
    "    sc.spark_context.addFile('temp.out')\n",
    " \n",
    "    with get_elapsed_time():\n",
    "        predict = set_model_conf(is_training=IS_TRAINING, use_field=USE_FIELD)\n",
    " \n",
    "        columns = ['csno', 'cntn_id', 'event_dttm', 'score', 'label']\n",
    "        score_df = service_dataset.mapPartitions(lambda x: predict(x)).toDF(columns)\n",
    "        score_df = score_df.withColumn('part_dt', F.lit(BASE_DATE))\n",
    " \n",
    "    with get_elapsed_time():\n",
    "        mode = 'append' if IS_APPENDING else 'overwrite'\n",
    "        score_df.write.saveAsTable('%s.fm_score_for_service' % DATABASE, mode=mode, partitionBy='part_dt')\n",
    "        _ = delete_dir_or_file('temp.out')\n",
    " \n",
    "    sc.stop()\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    build_xlearn_dataset_for_service()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import xlearn as xl\n",
    "from sklearn.metrics import log_loss, roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from my_utils.misc import *\n",
    " \n",
    " \n",
    "def print_result(i, res, verbose=True):\n",
    "    string = '-' * 94\n",
    "    string = '\\n'.join([string, '<{}th model\\'s result>'.format(i + 1), '[parameter]'])\n",
    "    for k, v in res[str(i)]['param'].items():\n",
    "        string = ' '.join([string, '{}: {}'.format(k, v)])\n",
    "    string = '\\n'.join([string, '[evaluation metric]'])\n",
    "    for k, v in res[str(i)]['eval_metric'].items():\n",
    "        string = ' '.join([string, '{}: {:.4%}'.format(k, v)])\n",
    "    string = '\\n'.join([string, '[execution time] {:.4f} sec'.format(res[str(i)]['exec_time']), '-' * 94])\n",
    "    if verbose:\n",
    "        print(string)\n",
    " \n",
    " \n",
    "USE_THREE_FOLDS = True\n",
    "USE_TIME_BASED = True\n",
    "USE_FIELD = False\n",
    "SAMPLING_RATE = 0.1\n",
    "IS_NEGATIVE_DOWN_SAMPLING = False\n",
    "FEATURE_CATEGORIES = sorted(['user_demo', 'user_dtag', 'item_profile', 'item_context'])\n",
    " \n",
    "USING_THREAD_RATIO = 0.5\n",
    "N_ITER = 30\n",
    " \n",
    "DATA_DIR_NAME = 'data'\n",
    "HADOOP_MODEL_DIR_NAME = 'model'\n",
    " \n",
    " \n",
    "def train_xlearn_model():\n",
    "    train_set_type = 'train' if USE_THREE_FOLDS else 'train+valid'\n",
    "    test_set_type = 'valid' if USE_THREE_FOLDS else 'test'\n",
    "    train_sampling_type = '%dbp_neg_sampled' % int(\n",
    "        10000 * SAMPLING_RATE) if IS_NEGATIVE_DOWN_SAMPLING else '%dbp_sampled' % int(10000 * SAMPLING_RATE)\n",
    "    test_sampling_type = '%dbp_sampled' % int(10000 * SAMPLING_RATE)\n",
    "    source_table = 'time' if USE_TIME_BASED else 'user'\n",
    "    model_type = 'ffm' if USE_FIELD else 'fm'\n",
    "    feature_categories = ['all'] if FEATURE_CATEGORIES is None else FEATURE_CATEGORIES\n",
    " \n",
    "    _ = mkdir_in_hdfs(HADOOP_MODEL_DIR_NAME)\n",
    " \n",
    "    train_set_name = os.path.join(DATA_DIR_NAME, '{}_{}_based_{}_{}_dataset({}).libsvm'.format(\n",
    "        model_type, source_table, train_sampling_type, train_set_type, '+'.join(feature_categories)))\n",
    "    test_set_name = os.path.join(DATA_DIR_NAME, '{}_{}_based_{}_{}_dataset({}).libsvm'.format(\n",
    "        model_type, source_table, test_sampling_type, test_set_type, '+'.join(feature_categories)))\n",
    "    param_file_name = 'param_of_{}_{}_based_{}_{}_model({}).pkl'.format(\n",
    "        model_type, source_table, train_sampling_type, train_set_type, '+'.join(feature_categories))\n",
    "    model_name = '{}_{}_based_{}_{}_model({})'.format(\n",
    "        model_type, source_table, train_sampling_type, train_set_type, '+'.join(feature_categories))\n",
    " \n",
    "    model = xl.create_ffm() if USE_FIELD else xl.create_fm()\n",
    "    model.setOnDisk()\n",
    "    disable_norm = True\n",
    "    if disable_norm:\n",
    "        model.disableNorm()\n",
    "    model.setTrain(train_set_name)\n",
    "    model.setValidate(test_set_name)\n",
    "    model.setSigmoid()\n",
    " \n",
    "    default_param = {'task': 'binary', 'nthread': int(USING_THREAD_RATIO * multiprocessing.cpu_count()),\n",
    "                     'opt': 'adagrad', 'epoch': 10, 'stop_window': 3, 'metric': 'auc'}\n",
    " \n",
    "    if USE_THREE_FOLDS:\n",
    "        param_grid = {'lr': 0.2 * 2 ** np.linspace(-3, 1, 5), 'lambda': 0.002 * 2 ** np.linspace(-1, 3, 5),\n",
    "                      'k': 2 * 2 ** np.linspace(0, 4, 5, dtype=int)}\n",
    "        param_list = [{'lr': 0.2, 'lambda': 0.002, 'k': 4}]\n",
    "        if N_ITER > 0:\n",
    "            param_list += list(ParameterSampler(param_grid, n_iter=N_ITER, random_state=42))\n",
    " \n",
    "        with open(test_set_name, 'r') as file:\n",
    "            y_actual = np.array([int(line[0]) for line in file])\n",
    " \n",
    "        res = {}\n",
    "        criteria = 'log_loss'\n",
    "        max_val = None\n",
    " \n",
    "        for i, param in enumerate(param_list):\n",
    "            start_time = time.perf_counter()\n",
    " \n",
    "            default_param.update(param)\n",
    "            model.fit(default_param, 'temp.out')\n",
    "            model.setTest(test_set_name)\n",
    "            model.predict('temp.out', 'temp.txt')\n",
    " \n",
    "            with open('temp.txt', 'r') as file:\n",
    "                score = np.array([float(line) for line in file])\n",
    " \n",
    "            res[str(i)] = {'param': {'lr': default_param['lr'], 'lambda': default_param['lambda'],\n",
    "                                     'k': default_param['k']},\n",
    "                           'eval_metric': {'log_loss': -1.0 * log_loss(y_actual, score),\n",
    "                                           'auroc': roc_auc_score(y_actual, score),\n",
    "                                           'auprc': average_precision_score(y_actual, score)},\n",
    "                           'exec_time': time.perf_counter() - start_time}\n",
    " \n",
    "            if i == 0:\n",
    "                max_val = res[str(i)]['eval_metric'][criteria]\n",
    " \n",
    "            if res[str(i)]['eval_metric'][criteria] >= max_val:\n",
    "                dump_pickle('temp.pkl', res[str(i)])\n",
    " \n",
    "                _ = delete_file_in_hdfs(os.path.join(HADOOP_MODEL_DIR_NAME, 'best_' + param_file_name))\n",
    "                _ = delete_file_in_hdfs(os.path.join(HADOOP_MODEL_DIR_NAME, model_name + '.out'))\n",
    "                _ = delete_dir_or_file(os.path.join(DATA_DIR_NAME, model_name + '.txt'))\n",
    " \n",
    "                _ = copy_local_to_hdfs('temp.pkl', os.path.join(HADOOP_MODEL_DIR_NAME, 'best_' + param_file_name))\n",
    "                _ = copy_local_to_hdfs('temp.out', os.path.join(HADOOP_MODEL_DIR_NAME, model_name + '.out'))\n",
    "                _ = cmd_executor('cp %s %s' % ('temp.txt', os.path.join(DATA_DIR_NAME, model_name + '.txt')))\n",
    " \n",
    "                _ = delete_dir_or_file('temp.pkl')\n",
    " \n",
    "            _ = delete_dir_or_file('temp.out')\n",
    "            _ = delete_dir_or_file('temp.txt')\n",
    " \n",
    "            print_result(i, res, verbose=True)\n",
    " \n",
    "        dump_pickle(os.path.join(DATA_DIR_NAME, 'all_' + param_file_name), res)\n",
    " \n",
    "    else:\n",
    "        with get_elapsed_time():\n",
    "            _ = copy_hdfs_to_local(os.path.join(HADOOP_MODEL_DIR_NAME, 'best_' + param_file_name), 'temp.pkl')\n",
    "            param = load_pickle('temp.pkl')['param']\n",
    " \n",
    "            default_param.update(param)\n",
    "            model.fit(default_param, 'temp.out')\n",
    " \n",
    "            _ = delete_file_in_hdfs(os.path.join(HADOOP_MODEL_DIR_NAME, model_name + '.out'))\n",
    "            _ = copy_local_to_hdfs('temp.out', os.path.join(HADOOP_MODEL_DIR_NAME, model_name + '.out'))\n",
    " \n",
    "            _ = delete_dir_or_file('temp.pkl')\n",
    "            _ = delete_dir_or_file('temp.out')\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    train_xlearn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from subprocess import PIPE, Popen\n",
    " \n",
    " \n",
    "def cmd_executor(args, delimit=' '):\n",
    "    if not isinstance(args, list):\n",
    "        args = args.split(delimit)\n",
    "    process = Popen(args, stdout=PIPE, stderr=PIPE)\n",
    "    res = []\n",
    "    while process.poll() is None:\n",
    "        line = process.stdout.readline()\n",
    "        if line != b'' and line.endswith(b'\\n'):\n",
    "            res.append(line[:-1])\n",
    "    stdout, stderr = process.communicate()\n",
    "    res += stdout.split(b'\\n')\n",
    "    if stderr != b'':\n",
    "        res += stderr.split(b'\\n')\n",
    "    res.remove(b'')\n",
    "    res = '\\n'.join([x.decode() for x in res]).strip()\n",
    "    return res\n",
    " \n",
    " \n",
    "def concat_text_file(source_files_path, target_file_path, n_chunks=1):\n",
    "    n_files = len(source_files_path) // n_chunks\n",
    " \n",
    "    for i in range(n_chunks):\n",
    "        file_number = '-' + '%05d' % i if n_chunks > 1 else ''\n",
    "        file_name = target_file_path[:target_file_path.rfind('.')] + file_number + target_file_path[\n",
    "                                                                                   target_file_path.rfind('.'):]\n",
    "        with open(file_name, 'wb') as wfd:\n",
    "            start_num = i * n_files\n",
    "            end_num = (i + 1) * n_files if i < (n_chunks - 1) else len(source_files_path)\n",
    "            for f in source_files_path[start_num:end_num]:\n",
    "                with open(f, 'rb') as fd:\n",
    "                    shutil.copyfileobj(fd, wfd)\n",
    " \n",
    " \n",
    "def copy_hdfs_to_local(source_path, target_path='.'):\n",
    "    return cmd_executor('hdfs dfs -copyToLocal %s %s' % (source_path, target_path))\n",
    " \n",
    " \n",
    "def copy_local_to_hdfs(source_path, target_path='.'):\n",
    "    return cmd_executor('hdfs dfs -put %s %s' % (source_path, target_path))\n",
    " \n",
    " \n",
    "def delete_dir_or_file(path):\n",
    "    try:\n",
    "        shutil.rmtree(path)\n",
    "    except:\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except:\n",
    "            pass\n",
    " \n",
    " \n",
    "def delete_file_in_hdfs(file_path, skip_trash=True):\n",
    "    skip_trash_str = ' -skipTrash' if skip_trash else ''\n",
    "    return cmd_executor('hdfs dfs -rm -r%s %s' % (skip_trash_str, file_path))\n",
    " \n",
    " \n",
    "def dump_pickle(file_path, obj):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    " \n",
    " \n",
    "def get_cleaned_hql(query):\n",
    "    comment = re.compile(r'--+.*[\\n]+')\n",
    "    query = re.sub(comment, '', query)\n",
    "    ws = re.compile(r'[\\t\\n]+')\n",
    "    return re.sub(ws, ' ', query)\n",
    " \n",
    " \n",
    "@contextmanager\n",
    "def get_elapsed_time(format_string='Elapsed time: %.4f sec', verbose=True):\n",
    "    start_time = time.perf_counter()\n",
    "    yield\n",
    "    elapsed_time = time.perf_counter() - start_time\n",
    "    if verbose:\n",
    "        print(format_string % elapsed_time)\n",
    " \n",
    " \n",
    "def get_employee_id():\n",
    "    return str(re.search(r'.+(\\d{6}).?', os.getcwd()).group(1))\n",
    " \n",
    " \n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    " \n",
    " \n",
    "def mkdir_in_hdfs(dir_path, create_parents=True):\n",
    "    create_parents_arg = '-p ' if create_parents else ''\n",
    "    return cmd_executor('hdfs dfs -mkdir %s%s' % (create_parents_arg, dir_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.ml.pipeline import Transformer\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    " \n",
    " \n",
    "class AsDenseTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(AsDenseTransformer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    " \n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    " \n",
    "    def _transform(self, dataset):\n",
    "        output_col = self.getOutputCol()\n",
    "        input_col = dataset[self.getInputCol()]\n",
    "        as_dense = F.udf(lambda s: DenseVector(s.toArray()), VectorUDT())\n",
    "        return dataset.withColumn(output_col, as_dense(input_col))\n",
    " \n",
    " \n",
    "def adjust_date_format(column_name, is_monthly=False, adding_periods=0):\n",
    "    if is_monthly:\n",
    "        return F.date_format(F.add_months(\n",
    "            F.to_date(F.substring(F.col(column_name), 1, 6), 'yyyyMM'), adding_periods), 'yyyyMM')\n",
    "    else:\n",
    "        return F.date_format(F.date_add(F.to_date(F.col(column_name), 'yyyyMMdd'), adding_periods), 'yyyyMMdd')\n",
    " \n",
    " \n",
    "def cast_data_type(df, column_names, data_type):\n",
    "    for column_name in list(column_names):\n",
    "        df = df.withColumn(column_name, df[column_name].cast(data_type))\n",
    "    return df\n",
    " \n",
    " \n",
    "def get_null_ratio(df, feature_names):\n",
    "    tot_count = df.count()\n",
    "    null_ratios = {}\n",
    "    for feature_name in list(feature_names):\n",
    "        null_ratios[feature_name] = df.filter(F.col(feature_name).isNull()).count() / tot_count\n",
    "    return null_ratios\n",
    " \n",
    " \n",
    "def get_spark_conf(spark_context, conf):\n",
    "    return spark_context._conf.get(conf)\n",
    " \n",
    " \n",
    "def get_total_n_executors_core(spark_context):\n",
    "    n_instances = int(get_spark_conf(spark_context, 'spark.executor.instances'))\n",
    "    n_cores_per_executor = int(get_spark_conf(spark_context, 'spark.executor.cores'))\n",
    "    return n_instances * n_cores_per_executor\n",
    " \n",
    " \n",
    "def sample_dataset(df, fraction, seed=42, is_negative_down_sampling=False, target_name=None, target_value=(0, 1)):\n",
    "    if fraction == 1.0:\n",
    "        sampled_df = df\n",
    "    elif not is_negative_down_sampling or target_name is None:\n",
    "        sampled_df = df.sample(False, fraction, seed=seed)\n",
    "    else:\n",
    "        pos_df = df.filter(F.col(target_name) == target_value[1])\n",
    "        neg_df = df.filter(F.col(target_name) == target_value[0]).sample(False, fraction, seed=seed)\n",
    "        sampled_df = pos_df.unionAll(neg_df)\n",
    "    return sampled_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
